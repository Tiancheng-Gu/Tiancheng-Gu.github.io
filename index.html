<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tiancheng Gu</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Crimson+Text:ital,wght@0,400;0,600;0,700;1,400&family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
      tailwind.config = {
        theme: {
          extend: {
            colors: {
              background: '#fefffe',
              foreground: '#0f172a',
              primary: '#1e293b',
              accent: '#d4a562',
              'accent-dark': '#b88a4d',
              muted: '#64748b',
            },
            fontFamily: {
              sans: ['Inter', 'sans-serif'],
              serif: ['Crimson Text', 'serif'],
            }
          }
        }
      }
    </script>
</head>
<body class="bg-background text-foreground antialiased font-sans">
    <!-- Top Navigation -->
    <nav class="top-nav glass fixed top-0 left-0 right-0 z-50 transition-all duration-300">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
            <div class="flex justify-between items-center h-16 lg:h-20">
                <!-- Logo/Name -->
                <div class="flex-shrink-0">
                    <a href="#" class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200">
                        Tiancheng Gu
                    </a>
                </div>

                <!-- Desktop Navigation -->
                <div class="hidden lg:block">
                    <div class="ml-10 flex items-center space-x-8">
                        <ul class="nav-links flex space-x-8">
                            <li><a href="#homepage" class="nav-item">Homepage</a></li>
                            <li><a href="#about" class="nav-item">About Me</a></li>
                            <li><a href="#latest-news" class="nav-item">News</a></li>
                            <li><a href="#publications" class="nav-item">Publications</a></li>
                            <li><a href="#honors" class="nav-item">Honors</a></li>
                            <li><a href="#experiences" class="nav-item">Experiences</a></li>
                            <li><a href="#educations" class="nav-item">Education</a></li>
                            <li><a href="#services" class="nav-item">Service</a></li>
                        </ul>
                    </div>
                </div>

                <!-- Mobile Menu Button -->
                <div class="lg:hidden flex items-center">
                    <button class="mobile-menu-btn p-2 rounded-md text-neutral-600 hover:text-primary">
                        <i class="fas fa-bars"></i>
                    </button>
                </div>
            </div>
            
            <!-- Mobile Menu -->
            <div class="mobile-menu hidden lg:hidden absolute top-16 left-0 w-full bg-white/95 backdrop-blur-md border-b border-neutral-200 shadow-lg" id="mobile-menu">
                <ul class="flex flex-col py-4 px-4 space-y-4">
                    <li><a href="#homepage" class="mobile-nav-item block text-neutral-600 hover:text-primary font-medium">Homepage</a></li>
                    <li><a href="#about" class="mobile-nav-item block text-neutral-600 hover:text-primary font-medium">About Me</a></li>
                    <li><a href="#latest-news" class="mobile-nav-item block text-neutral-600 hover:text-primary font-medium">News</a></li>
                    <li><a href="#publications" class="mobile-nav-item block text-neutral-600 hover:text-primary font-medium">Publications</a></li>
                    <li><a href="#honors" class="mobile-nav-item block text-neutral-600 hover:text-primary font-medium">Honors</a></li>
                    <li><a href="#experiences" class="mobile-nav-item block text-neutral-600 hover:text-primary font-medium">Experiences</a></li>
                    <li><a href="#services" class="mobile-nav-item block text-neutral-600 hover:text-primary font-medium">Service</a></li>
                    <li><a href="#educations" class="mobile-nav-item block text-neutral-600 hover:text-primary font-medium">Education</a></li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Main Content Grid -->
    <main class="min-h-screen pt-20 lg:pt-28 pb-12">
        <div class="w-full max-w-[1380px] mx-auto px-4 sm:px-6 lg:px-8">
            <div class="grid-layout">
                
                <!-- Left Column - Profile -->
                <div class="profile-column">
                    <div class="profile-card">
                        <!-- Profile Image -->
                        <div class="profile-image-container mb-6 mx-auto max-w-[180px]">
                            <img src="assets/profile.jpg" id="profile-img" alt="Tiancheng Gu" class="w-full h-auto rounded-xl shadow-sm bg-gray-200">
                        </div>

                        <!-- Name and Title -->
                        <div class="text-center mb-6">
                            <h1 class="text-3xl font-serif font-bold text-primary mb-2">Tiancheng Gu | È°æÂ§©Êâø</h1>
                            <p class="text-lg text-accent font-medium mb-1">Ph.D. Student</p>
                            <p class="text-neutral-600 mb-2">The University of Sydney</p>
                        </div>

                        <!-- Contact Links -->
                        <div class="social-links flex flex-wrap justify-center gap-3 mb-6">
                            <a href="mailto:tiancheng.gu@sydney.edu.au" class="social-btn" title="Email"><i class="fas fa-envelope"></i></a>
                            <a href="https://linkedin.com/business-manager-api/bzmEnterpriseAccessCookie/in/tiancheng-gu-721437255/?trk=PROFILE_DROP_DOWN" target="_blank" class="social-btn" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
                            <!-- <a href="#" target="_blank" class="social-btn" title="CV"><i class="fas fa-file-pdf"></i></a> -->
                            <a href="https://scholar.google.com/citations?user=9etrpbYAAAAJ" target="_blank" class="social-btn" title="Google Scholar"><i class="fas fa-graduation-cap"></i></a>
                            <a href="https://github.com/GaryGuTC" target="_blank" class="social-btn" title="GitHub"><i class="fab fa-github"></i></a>
                            <!-- <a href="#" target="_blank" class="social-btn" title="ResearchGate"><i class="fab fa-researchgate"></i></a> -->
                            <!-- <a href="https://garygutc.github.io" target="_blank" class="social-btn" title="Blog"><i class="fas fa-blog"></i></a> -->
                        </div>
                        
                        <!-- Research Interests -->
                         <div class="research-interests-card bg-neutral-100 rounded-xl p-5 mb-6">
                            <h3 class="font-serif font-bold text-lg text-primary mb-4 pb-2 border-b border-neutral-200 flex items-center">
                                <i class="fas fa-lightbulb text-accent mr-2 text-base"></i> Research Interests
                            </h3>
                            <ul class="space-y-3">
                                <li class="flex items-center text-neutral-700 hover:text-primary transition-colors">
                                    <i class="fas fa-star text-accent/80 mr-3 w-5 text-center text-sm"></i>
                                    <span class="text-sm font-medium">Large-scale Synthetic Data</span>
                                </li>
                                <li class="flex items-center text-neutral-700 hover:text-primary transition-colors">
                                    <i class="fas fa-star text-accent/80 mr-3 w-5 text-center text-sm"></i>
                                    <span class="text-sm font-medium">CPT&PT of LVLMs in Video Understanding</span>
                                </li>
                                <!-- <li class="flex items-center text-neutral-700 hover:text-primary transition-colors">
                                    <i class="fas fa-star text-accent/80 mr-3 w-5 text-center text-sm"></i>
                                    <span class="text-sm font-medium">MLLM-based Embedding Models</span>
                                </li> -->
                                <li class="flex items-center text-neutral-700 hover:text-primary transition-colors">
                                    <i class="fas fa-star text-accent/80 mr-3 w-5 text-center text-sm"></i>
                                    <span class="text-sm font-medium">CLIP Pre-Training</span>
                                </li>
                            </ul>
                        </div>

                    </div>
                </div>

                <!-- Right Column - Content -->
                <div class="content-column space-y-12">
                    
                    <!-- About Section -->
                    <section id="homepage" class="scroll-mt-24">
                        <span id="about"></span> <!-- Anchor for About link -->
                        <div class="section-header mb-6">
                            <h2 class="text-3xl font-serif font-bold text-primary">About Me</h2>
                        </div>
                        <div class="prose max-w-none text-neutral-600 text-justify formal-font">
                            <p class="mb-4">I am a third-year Ph.D. student at the University of Sydney, under the supervision of A/Professor <a href="https://weidong-tom-cai.github.io/" class="text-accent hover:text-accent-dark">Weidong Cai</a>. I previously completed a B.Eng. (Honours) in 2022 at the Australian National University, where I was advised by Professor <a href="https://users.cecs.anu.edu.au/~hongdong/" class="text-accent hover:text-accent-dark">Hongdong Li</a>. I also hold a B.Sci. in Computer Science, awarded in 2021 by the University of Melbourne.</p>
                            
                            <!-- <div class="highlight-box bg-accent/10 border-l-4 border-accent p-4 rounded-r-lg text-left font-highlight">
                                <p><strong><i class="fas fa-lightbulb text-accent mr-2"></i>I am actively seeking job opportunities, </strong> and if you are interested in me, feel free to reach out to me via email: <u>gtcivy01@outlook.com</u> or WeChat: <u>zjg_Gutiancheng_haha</u>.</p>
                            </div> -->
                        </div>
                    </section>

                    <!-- News Section -->
                    <section id="latest-news" class="scroll-mt-24">
                        <div class="section-header mb-6 flex justify-between items-center">
                            <h2 class="text-3xl font-serif font-bold text-primary">üî• News</h2>
                        </div>
                        <div class="news-scroll-wrapper">
                            <div id="news-container" class="news-list">
                                <!-- News items embedded directly -->
                                <div class="news-item">
                                    <span class="news-date">2025.10</span>
                                    <div class="news-content">
                                        üéâ <a href="https://garygutc.github.io/UniME-v2" target="_blank">UniME-V2</a> has been accepted by AAAI 2026 as <u>Oral Presentation</u>!
                                    </div>
                                </div>
                                <div class="news-item">
                                    <span class="news-date">2025.07</span>
                                    <div class="news-content">
                                        üéâ <a href="https://garygutc.github.io/UniME" target="_blank">UniME</a> and <a href="https://garygutc.github.io/RealSyn" target="_blank">RealSyn</a> have been accepted by ACM MM 2025! <u>All members oral</u>!
                                    </div>
                                </div>
                                <div class="news-item">
                                    <span class="news-date">2024.10</span>
                                    <div class="news-content">
                                        üéâ <a href="https://arxiv.org/abs/2408.09441" target="_blank">CLIP-CID</a> has been accepted by AAAI 2025!
                                    </div>
                                </div>
                                <div class="news-item">
                                    <span class="news-date">2024.10</span>
                                    <div class="news-content">
                                        üéâ <a href="https://arxiv.org/abs/2411.13025" target="_blank">ORID</a> has been accepted by WACV 2025 as <u>Oral Presentation</u>!
                                    </div>
                                </div>
                                <div class="news-item">
                                    <span class="news-date">2024.09</span>
                                    <div class="news-content">
                                        üéâ <a href="https://github.com/deepglint/RWKV-CLIP" target="_blank">RWKV-CLIP</a> has been accepted by EMNLP 2024 Main!
                                    </div>
                                </div>
                                <div class="news-item">
                                    <span class="news-date">2024.05</span>
                                    <div class="news-content">
                                        üéâ <a href="https://arxiv.org/abs/2404.13039" target="_blank">LaPA</a> has been accepted by CVPR 2024 Workshop!
                                    </div>
                                </div>
                                <div class="news-item">
                                    <span class="news-date">2023.10</span>
                                    <div class="news-content">
                                        üéâ <a href="https://github.com/GaryGuTC/COMG_model" target="_blank">COMG</a> has been accepted by WACV 2024!
                                    </div>
                                </div>
                                <div class="news-item">
                                    <span class="news-date">2023.03</span>
                                    <div class="news-content">
                                        üéâ Started my Ph.D. journey at The University of Sydney!
                                    </div>
                                </div>
                            </div>
                        </div>
                    </section>

                    <!-- Publications Section -->
                    <section id="publications" class="scroll-mt-24">
                        <div class="section-header mb-6">
                            <h2 class="text-3xl font-serif font-bold text-primary">üìÑ Selected Publications</h2>
                            <p class="text-sm text-neutral-500 mt-1">(* means equal contribution, üìß means corresponding author)</p>
                        </div>
                        
                        <div class="publications-list">
                            <!-- 2026 -->
                            <div class="pub-year-group">
                                <h3 class="pub-year-header">2026</h3>
                                <div class="pub-list-container">
                                    <div class="pub-card">
                                        <div class="pub-card-summary">
                                            <h4 class="pub-title-text">
                                                <span class="pub-venue-tag tag-conference">AAAI'26</span>
                                                <span class="pub-badge-highlight">Oral</span>
                                                UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning
                                            </h4>
                                            <div class="pub-authors"><strong class="text-accent text-lg">Tiancheng Gu</strong>*, Kaicheng Yang*, Kaichen Zhang, Xiang An, Yueyi Zhangüìß, Weidong Cai, Jiankang Dengüìß, Lidong Bing</div>
                                        </div>
                                        <div class="pub-card-details">
                                            <div class="pub-venue-info">
                                                <span>AAAI Conference on Artificial Intelligence (AAAI'26)</span>
                                                <span class="ccf-rank ccf-a">CORE A*</span>
                                                <span class="ccf-rank ccf-a">CCF-A</span>
                                            </div>
                                            <div class="pub-links">
                                                <a href="https://arxiv.org/pdf/2510.13515" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-file-pdf"></i> Paper
                                                </a>
                                                <a href="https://garygutc.github.io/UniME-v2" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-globe"></i> Webpage
                                                </a>
                                                <a href="https://github.com/GaryGuTC/UniME-v2" target="_blank" class="pub-link-btn">
                                                    <i class="fab fa-github"></i> Code
                                                </a>
                                                <a href="https://mp.weixin.qq.com/s/LnSqRpnxH33lzmCAFeGfHA" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-newspaper"></i> ÈáèÂ≠ê‰Ωç
                                                </a>
                                            </div>

                                            <!-- Publication Image (Optional) -->
                                            <div class="pub-image-container">
                                                <img src="assets/papers/UniME_v2.jpg" alt="UniME-V2 Overview">
                                            </div>
                                            
                                            <!-- Publication Abstract/Description (Optional) -->
                                            <div class="pub-abstract">
                                                <div class="pub-abstract-title">TL;DR</div>
                                                <p>UniME-V2 leverages Multimodal LLMs as judges to identify high-quality hard negatives and learns fine-grained semantic distinctions through soft-label distribution alignment, achieving SOTA performance in universal multimodal retrieval.</p>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="pub-card">
                                        <div class="pub-card-summary">
                                            <h4 class="pub-title-text">
                                                <span class="pub-venue-tag tag-review">Under Review</span>
                                                DanQing-100M: A Large-scale Chinese Vision-Language Pre-training Dataset
                                            </h4>
                                            <div class="pub-authors">Hengyu Shen*, <strong class="text-accent text-lg">Tiancheng Gu</strong>*, Bin Qin, Shuo Tan, Zelong Sun Jun Wang, Nan Wu, Xiang An, Ziyong Feng, Kaicheng Yangüìß</div>
                                        </div>
                                        <div class="pub-card-details">
                                            
                                            <div class="pub-venue-info">
                                                <span>Technique Report</span>
                                            </div>
                                            <div class="pub-links">
                                                <a href="https://arxiv.org/pdf/2601.10305" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-file-pdf"></i> Paper
                                                </a>
                                                <a href="https://deepglint.github.io/DanQing" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-globe"></i> Webpage
                                                </a>
                                                <a href="https://github.com/deepglint/DanQing" target="_blank" class="pub-link-btn">
                                                    <i class="fab fa-github"></i> Code
                                                </a>
                                            </div>

                                            <!-- Publication Image (Optional) -->
                                            <div class="pub-image-container">
                                                <img src="assets/papers/Danqing.jpg" alt="DanQing-100M Overview">
                                            </div>
                                            
                                            <!-- Publication Abstract/Description (Optional) -->
                                            <div class="pub-abstract">
                                                <div class="pub-abstract-title">TL;DR</div>
                                                <p>DanQing is a curated dataset of 100 million Chinese image-text pairs sourced from 2024‚Äì2025 web data. By implementing a rigorous multi-stage filtering pipeline that retains only the top 9.54% of raw data, it achieves superior quality and temporal relevance over existing benchmarks. Models pre-trained on DanQing achieve state-of-the-art (SOTA) performance in zero-shot classification, cross-modal retrieval, and Chinese-centric multimodal reasoning.</p>
                                            </div>
                                        </div>
                                    </div>
                                    <!-- <div class="pub-card">
                                        <div class="pub-card-summary">
                                            <h4 class="pub-title-text">
                                                <span class="pub-venue-tag tag-review">Under Review</span>
                                                HieraDoc: A Hierarchical Benchmark for Document Retrieval-Augmented Generation
                                            </h4>
                                            <div class="pub-authors">Jun Wang*, <strong class="text-accent text-lg">Tiancheng Gu</strong>*, Shuo Tan, Yifan Zhang, Hengyu Shen, Xiaoxing Hu, Weidong Cai, Ziyong Feng, Xiang An, Kaicheng Yangüìß</div>
                                        </div>
                                        <div class="pub-card-details">
                                            Publication Image (Optional)
                                            <div class="pub-image-container">
                                                <img src="assets/publications/placeholder/hieradoc.png" alt="HieraDoc Overview">
                                            </div>
                                            
                                            Publication Abstract/Description (Optional)
                                            <div class="pub-abstract">
                                                <div class="pub-abstract-title">Abstract</div>
                                                <p>HieraDoc presents a hierarchical benchmark for document retrieval-augmented generation, enabling more accurate and context-aware document understanding.</p>
                                            </div>
                                            
                                            <div class="pub-venue-info">
                                                <span>Under Review</span>
                                            </div>
                                        </div>
                                    </div> -->
                                </div>
                            </div>
                            
                            <!-- 2025 -->
                            <div class="pub-year-group">
                                <h3 class="pub-year-header">2025</h3>
                                <div class="pub-list-container">
                                    <div class="pub-card">
                                        <div class="pub-card-summary">
                                            <h4 class="pub-title-text">
                                                <span class="pub-venue-tag tag-conference">ACM MM'25</span>
                                                <!-- <span class="pub-badge-highlight">Oral</span> -->
                                                Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs
                                            </h4>
                                            <div class="pub-authors"><strong class="text-accent text-lg">Tiancheng Gu</strong>*, Kaicheng Yang*, Yanzhao Zhang, Yingda chen, Dingkun Long, Weidong Cai, JianKang Dengüìß</div>
                                        </div>
                                        <div class="pub-card-details">
                                            
                                            <div class="pub-venue-info">
                                                <span>ACM International Conference on Multimedia (ACM MM'25)</span>
                                                <span class="ccf-rank ccf-a">CORE A*</span>
                                                <span class="ccf-rank ccf-a">CCF-A</span>
                                            </div>
                                            <div class="pub-links">
                                                <a href="https://arxiv.org/pdf/2504.17432" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-file-pdf"></i> Paper
                                                </a>
                                                <a href="https://garygutc.github.io/UniME" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-globe"></i> Webpage
                                                </a>
                                                <a href="https://github.com/deepglint/UniME" target="_blank" class="pub-link-btn">
                                                    <i class="fab fa-github"></i> Code
                                                </a>
                                                <a href="https://mp.weixin.qq.com/s/d4huTt19EWBkvtzAHDI1uQ" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-newspaper"></i> ÈáèÂ≠ê‰Ωç
                                                </a>
                                            </div>

                                            <!-- Publication Image (Optional) -->
                                            <div class="pub-image-container">
                                                <img src="assets/papers/UniME.jpg" alt="UniME Overview">
                                            </div>
                                            
                                            <!-- Publication Abstract/Description (Optional) -->
                                            <div class="pub-abstract">
                                                <div class="pub-abstract-title">TL;DR</div>
                                                <p>UniME is a novel two-stage framework that empowers Multimodal Large Language Models (MLLMs) to learn universal and discriminative representations for diverse downstream tasks. By leveraging textual discriminative knowledge distillation and hard negative enhanced instruction tuning, UniME overcomes the limitations of traditional models (like CLIP‚Äôs token truncation and bag-of-words behavior), achieving state-of-the-art (SOTA) performance on the MMEB benchmark and various retrieval tasks.</p>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="pub-card">
                                        <div class="pub-card-summary">
                                            <h4 class="pub-title-text">
                                                <span class="pub-venue-tag tag-conference">ACM MM'25</span>
                                                <!-- <span class="pub-badge-highlight">Oral</span> -->
                                                RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm
                                            </h4>
                                            <div class="pub-authors"><strong class="text-accent text-lg">Tiancheng Gu</strong>*, Kaicheng Yang*, Chaoyi Zhang, Yin Xie, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Caiüìß, Jiankang Dengüìß</div>
                                        </div>
                                        <div class="pub-card-details">
                                            
                                            <div class="pub-venue-info">
                                                <span>ACM International Conference on Multimedia (ACM MM'25)</span>
                                                <span class="ccf-rank ccf-a">CORE A*</span>
                                                <span class="ccf-rank ccf-a">CCF-A</span>
                                            </div>
                                            <div class="pub-links">
                                                <a href="https://arxiv.org/pdf/2502.12513" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-file-pdf"></i> Paper
                                                </a>
                                                <a href="https://garygutc.github.io/RealSyn" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-globe"></i> Webpage
                                                </a>
                                                <a href="https://github.com/deepglint/RealSyn" target="_blank" class="pub-link-btn">
                                                    <i class="fab fa-github"></i> Code
                                                </a>
                                                <a href="https://mp.weixin.qq.com/s/CnuVZMSkYHpFwnfaLZ-3qA" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-newspaper"></i> ÈáèÂ≠ê‰Ωç
                                                </a>
                                            </div>

                                            <!-- Publication Image (Optional) -->
                                            <div class="pub-image-container">
                                                <img src="assets/papers/Realsyn.jpg" alt="RealSyn Overview">
                                            </div>
                                            
                                            <!-- Publication Abstract/Description (Optional) -->
                                            <div class="pub-abstract">
                                                <div class="pub-abstract-title">TL;DR</div>
                                                <p>RealSyn is a scalable paradigm designed to unlock the potential of unpaired multimodal interleaved documents for vision-language pre-training. By integrating a hierarchical retrieval of realistic texts with LLM-based synthetic caption generation, it constructs a high-quality, semantically balanced dataset of up to 100 million pairs. Models pre-trained on RealSyn consistently achieve state-of-the-art (SOTA) performance across zero-shot transfer, robustness, and retrieval benchmarks, outperforming traditional datasets like LAION and YFCC.</p>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="pub-card">
                                        <div class="pub-card-summary">
                                            <h4 class="pub-title-text">
                                                <span class="pub-venue-tag tag-conference">AAAI'25</span>
                                                CLIP-CID: Efficient CLIP Distillation via Cluster-Instance Discrimination
                                            </h4>
                                            <div class="pub-authors">Kaicheng Yang*, <strong class="text-accent text-lg">Tiancheng Gu</strong>*, Xiang An, Haiqiang Jiang, Xiangzi Dai, Ziyong Feng, Weidong Caiüìß, Jiankang Dengüìß</div>
                                        </div>
                                        <div class="pub-card-details">
                                            
                                            <div class="pub-venue-info">
                                                <span>AAAI Conference on Artificial Intelligence (AAAI'25)</span>
                                                <span class="ccf-rank ccf-a">CORE A*</span>
                                                <span class="ccf-rank ccf-a">CCF-A</span>
                                            </div>
                                            <div class="pub-links">
                                                <a href="https://arxiv.org/abs/2408.09441" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-file-pdf"></i> Paper
                                                </a>
                                                <a href="https://zhuanlan.zhihu.com/p/13925507766" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-newspaper"></i> Áü•‰πé
                                                </a>
                                                <a href="https://mp.weixin.qq.com/s/vb1-nm-PXSJD5Qt-qIifyg" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-newspaper"></i> Êô∫ËØ≠ËßÜÁïå
                                                </a>
                                            </div>

                                            <!-- Publication Image (Optional) -->
                                            <div class="pub-image-container">
                                                <img src="assets/papers/CLIP_CID.png" alt="CLIP-CID Overview" class="small-image">
                                            </div>
                                            
                                            <!-- Publication Abstract/Description (Optional) -->
                                            <div class="pub-abstract">
                                                <div class="pub-abstract-title">TL;DR</div>
                                                <p>CLIP-CID is an efficient distillation framework for vision-language models that utilizes semantic balancing to prune redundant training data and cluster-instance discrimination to achieve state-of-the-art performance with lower computational costs.</p>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="pub-card">
                                        <div class="pub-card-summary">
                                            <h4 class="pub-title-text">
                                                <span class="pub-venue-tag tag-conference">WACV'25</span>
                                                <span class="pub-badge-highlight">Oral</span>
                                                ORID: Organ-Regional Information Driven Framework for Radiology Report Generation
                                            </h4>
                                            <div class="pub-authors"><strong class="text-accent text-lg">Tiancheng Gu</strong>*, Kaicheng Yang*, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Caiüìß</div>
                                        </div>
                                        <div class="pub-card-details">
                                            
                                            <div class="pub-venue-info">
                                                <span>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV'25)</span>
                                                <span class="ccf-rank ccf-b">CORE A</span>
                                            </div>
                                            <div class="pub-links">
                                                <a href="https://arxiv.org/pdf/2411.13025" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-file-pdf"></i> Paper
                                                </a>
                                            </div>

                                            <!-- Publication Image (Optional) -->
                                            <div class="pub-image-container">
                                                <img src="assets/papers/Orid.jpg" alt="ORID Overview">
                                            </div>
                                            
                                            <!-- Publication Abstract/Description (Optional) -->
                                            <div class="pub-abstract">
                                                <div class="pub-abstract-title">TL;DR</div>
                                                <p>We propose ORID, an Organ-Regional Information Driven framework that enhances radiology report generation by fusing organ-specific diagnostic descriptions with visual features and employing a GNN-based importance analysis to focus on clinically relevant regions while filtering out noise.</p>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- 2024 -->
                            <div class="pub-year-group">
                                <h3 class="pub-year-header">2024</h3>
                                <div class="pub-list-container">
                                    <div class="pub-card">
                                        <div class="pub-card-summary">
                                            <h4 class="pub-title-text">
                                                <span class="pub-venue-tag tag-conference">EMNLP'24</span>
                                                RWKV-CLIP: A Robust Vision-Language Representation Learner
                                            </h4>
                                            <div class="pub-authors"><strong class="text-accent text-lg">Tiancheng Gu</strong>*, Kaicheng Yang*, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Caiüìß, Jiankang Dengüìß</div>
                                        </div>
                                        <div class="pub-card-details">
                                            
                                            <div class="pub-venue-info">
                                                <span>Conference on Empirical Methods in Natural Language Processing (EMNLP'24)</span>
                                                <span class="ccf-rank ccf-a">CORE A*</span>
                                                <span class="ccf-rank ccf-b">CCF-B</span>
                                            </div>
                                            <div class="pub-links">
                                                <a href="https://arxiv.org/pdf/2406.06973" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-file-pdf"></i> Paper
                                                </a>
                                                <a href="https://github.com/deepglint/RWKV-CLIP" target="_blank" class="pub-link-btn">
                                                    <i class="fab fa-github"></i> Code
                                                </a>
                                                <a href="https://zhuanlan.zhihu.com/p/710382654a" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-newspaper"></i> Áü•‰πé
                                                </a>
                                                <a href="https://mp.weixin.qq.com/s/0sDHeN3QwF6AdFx3QyOvRw" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-newspaper"></i> ÂßãÊô∫AI
                                                </a>
                                            </div>

                                            <!-- Publication Image (Optional) -->
                                            <div class="pub-image-container">
                                                <img src="assets/papers/RWKV_CLIP.jpg" alt="RWKV-CLIP Overview">
                                            </div>
                                            
                                            <!-- Publication Abstract/Description (Optional) -->
                                            <div class="pub-abstract">
                                                <div class="pub-abstract-title">TL;DR</div>
                                                <p>RWKV-CLIP is an efficient and robust vision-language model that replaces traditional Transformer backbones with an RWKV-driven architecture and incorporates an LLM-based data refinement framework to achieve SOTA performance with significantly reduced memory and inference costs.</p>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="pub-card">
                                        <div class="pub-card-summary">
                                            <h4 class="pub-title-text">
                                                <span class="pub-venue-tag tag-workshop">CVPR Workshop'24</span>
                                                <span class="pub-badge-highlight">Oral</span>
                                                LaPA: Latent Prompt Assist Model For Medical Visual Question Answering
                                            </h4>
                                            <div class="pub-authors"><strong class="text-accent text-lg">Tiancheng Gu</strong>, Kaicheng Yang, Dongnan Liu, Weidong Caiüìß</div>
                                        </div>
                                        <div class="pub-card-details">
                                            <!-- Publication Image (Optional) -->
                                            <div class="pub-image-container">
                                                <!-- <img src="assets/publications/placeholder/lapa.png" alt="LaPA Overview"> -->
                                            </div>
                                            
                                            <!-- Publication Abstract/Description (Optional) -->
                                            <div class="pub-abstract">
                                                <div class="pub-abstract-title">TL;DR</div>
                                                <!-- <p>LaPA proposes a latent prompt assist model for medical visual question answering, enhancing the understanding of medical images through learned prompt representations.</p> -->
                                            </div>
                                            
                                            <div class="pub-venue-info">
                                                <span>IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop (CVPR'24 Workshop)</span>
                                            </div>
                                            <div class="pub-links">
                                                <a href="https://arxiv.org/pdf/2404.13039" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-file-pdf"></i> Paper
                                                </a>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="pub-card">
                                        <div class="pub-card-summary">
                                            <h4 class="pub-title-text">
                                                <span class="pub-venue-tag tag-conference">WACV'24</span>
                                                Complex Organ Mask Guided Radiology Report Generation
                                            </h4>
                                            <div class="pub-authors"><strong class="text-accent text-lg">Tiancheng Gu</strong>, Dongnan Liu, Zhiyuan Li, Weidong Caiüìß</div>
                                        </div>
                                        <div class="pub-card-details">
                                            <!-- Publication Image (Optional) -->
                                            <div class="pub-image-container">
                                                <!-- <img src="assets/publications/placeholder/comg.png" alt="COMG Overview"> -->
                                            </div>
                                            
                                            <!-- Publication Abstract/Description (Optional) -->
                                            <div class="pub-abstract">
                                                <div class="pub-abstract-title">TL;DR</div>
                                                <!-- <p>Complex Organ Mask Guided radiology report generation leverages organ segmentation masks to improve the accuracy and detail of generated medical reports.</p> -->
                                            </div>
                                            
                                            <div class="pub-venue-info">
                                                <span>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV'24)</span>
                                                <span class="ccf-rank ccf-b">CORE A</span>
                                            </div>
                                            <div class="pub-links">
                                                <a href="https://arxiv.org/pdf/2311.02329" target="_blank" class="pub-link-btn">
                                                    <i class="fas fa-file-pdf"></i> Paper
                                                </a>
                                                <a href="https://github.com/GaryGuTC/COMG_model" target="_blank" class="pub-link-btn">
                                                    <i class="fab fa-github"></i> Code
                                                </a>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </section>

                    <!-- Honors Section -->
                    <section id="honors" class="scroll-mt-24">
                        <div class="section-header mb-6">
                            <h2 class="text-3xl font-serif font-bold text-primary">üèÜ Honors & Awards</h2>
                        </div>
                        <div id="honors-container" class="honors-list space-y-4">
                            <div class="honor-item">
                                <div class="honor-year">2023</div>
                                <div class="honor-content">
                                    <h3>University of Sydney International Stipend Scholarship</h3>
                                    <p class="text-sm text-neutral-600">The University of Sydney</p>
                                </div>
                            </div>
                            <div class="honor-item">
                                <div class="honor-year">2023</div>
                                <div class="honor-content">
                                    <h3>University of Sydney International Tuition Fee Scholarship</h3>
                                    <p class="text-sm text-neutral-600">The University of Sydney</p>
                                </div>
                            </div>
                        </div>
                    </section>

                    <!-- Experiences Section -->
                    <section id="experiences" class="scroll-mt-24">
                        <div class="section-header mb-6">
                            <h2 class="text-3xl font-serif font-bold text-primary">üíº Experiences</h2>
                        </div>
                        <div class="timeline space-y-6">
                            <!-- Alibaba Qwen -->
                            <div class="timeline-item flex gap-4">
                                <div class="school-logo-container w-16 h-16 flex-shrink-0 bg-white rounded-lg shadow-sm p-2 flex items-center justify-center border border-neutral-100">
                                    <img src="assets/logos/qwen_logo.webp" alt="Alibaba" class="w-full h-full object-contain rounded">
                                </div>
                                <div class="timeline-content">
                                    <h3 class="font-semibold text-primary">Qwen Team</h3>
                                    <p class="text-sm text-accent font-medium">Tongyi Lab, Alibaba Group</p>
                                    <p class="text-sm text-neutral-500">2025.08 - Now</p>
                                </div>
                            </div>
                            
                            <!-- Miromind AI -->
                            <div class="timeline-item flex gap-4">
                                <div class="school-logo-container w-16 h-16 flex-shrink-0 bg-white rounded-lg shadow-sm p-2 flex items-center justify-center border border-neutral-100">
                                    <img src="assets/logos/miromind.jpg" alt="Miromind AI" class="w-full h-full object-contain rounded">
                                </div>
                                <div class="timeline-content">
                                    <h3 class="font-semibold text-primary">Miromind AI</h3>
                                    <p class="text-sm text-accent font-medium">Shanda Group</p>
                                    <p class="text-sm text-neutral-500">2025.05 - 2025.08</p>
                                </div>
                            </div>
                            
                            <!-- Alibaba ModelScope -->
                            <div class="timeline-item flex gap-4">
                                <div class="school-logo-container w-16 h-16 flex-shrink-0 bg-white rounded-lg shadow-sm p-2 flex items-center justify-center border border-neutral-100">
                                    <img src="assets/logos/modelscope.gif" alt="Alibaba" class="w-full h-full object-contain rounded">
                                </div>
                                <div class="timeline-content">
                                    <h3 class="font-semibold text-primary">ModelScope Team</h3>
                                    <p class="text-sm text-accent font-medium">Tongyi Lab, Alibaba Group</p>
                                    <p class="text-sm text-neutral-500">2025.02 - 2025.05</p>
                                </div>
                            </div>
                            
                            <!-- DeepGlint -->
                            <div class="timeline-item flex gap-4">
                                <div class="school-logo-container w-16 h-16 flex-shrink-0 bg-white rounded-lg shadow-sm p-2 flex items-center justify-center border border-neutral-100">
                                    <img src="assets/logos/deepglint.jpeg" alt="DeepGlint" class="w-full h-full object-contain rounded">
                                </div>
                                <div class="timeline-content">
                                    <h3 class="font-semibold text-primary">Glint Lab</h3>
                                    <p class="text-sm text-accent font-medium">DeepGlint Technologies Co. Ltd</p>
                                    <p class="text-sm text-neutral-500">2024.02 - 2025.02</p>
                                </div>
                            </div>
                            
                            <!-- Huawei -->
                            <div class="timeline-item flex gap-4">
                                <div class="school-logo-container w-16 h-16 flex-shrink-0 bg-white rounded-lg shadow-sm p-2 flex items-center justify-center border border-neutral-100">
                                    <img src="assets/logos/huawei.png" alt="Huawei" class="w-full h-full object-contain rounded">
                                </div>
                                <div class="timeline-content">
                                    <h3 class="font-semibold text-primary">Huawei Technologies Co. Ltd</h3>
                                    <p class="text-sm text-neutral-500">2021.03 - 2021.09</p>
                                </div>
                            </div>
                        </div>
                    </section>

                    <!-- Education Section -->
                    <section id="educations" class="scroll-mt-24">
                         <div class="section-header mb-6">
                            <h2 class="text-3xl font-serif font-bold text-primary">üéì Education</h2>
                        </div>
                        <div class="timeline space-y-6">
                            <!-- Ph.D. -->
                            <div class="timeline-item flex gap-4">
                                <div class="school-logo-container w-16 h-16 flex-shrink-0 bg-white rounded-lg shadow-sm p-2 flex items-center justify-center border border-neutral-100">
                                    <img src="assets/logos/Usyd.jpeg" alt="University of Sydney" class="w-full h-full object-contain rounded">
                                </div>
                                <div class="timeline-content">
                                    <h3 class="font-semibold text-primary">Ph.D. in Computer Science</h3>
                                    <p class="text-sm text-accent font-medium">The University of Sydney</p>
                                    <p class="text-sm text-neutral-500">2023.03 - Now</p>
                                    <p class="text-sm text-neutral-600 mt-1">
                                        Supervisor: <a href="https://weidong-tom-cai.github.io/" class="text-accent hover:text-accent-dark">A/Prof. Weidong Cai</a> & <a href="https://scholar.google.com/citations?user=JZzb8XUAAAAJ" class="text-accent hover:text-accent-dark">Dr. Dongnan Liu</a>
                                    </p>
                                </div>
                            </div>
                            
                            <!-- B.Eng. -->
                            <div class="timeline-item flex gap-4">
                                <div class="school-logo-container w-16 h-16 flex-shrink-0 bg-white rounded-lg shadow-sm p-2 flex items-center justify-center border border-neutral-100">
                                    <img src="assets/logos/ANU.jpg" alt="Australian National University" class="w-full h-full object-contain rounded">
                                </div>
                                <div class="timeline-content">
                                    <h3 class="font-semibold text-primary">B.Eng. in Computer Engineering (Honours)</h3>
                                    <p class="text-sm text-accent font-medium">The Australian National University</p>
                                    <p class="text-sm text-neutral-500">2022.03 - 2022.12</p>
                                    <p class="text-sm text-neutral-600 mt-1">
                                        Advisor: <a href="https://users.cecs.anu.edu.au/~hongdong/" class="text-accent hover:text-accent-dark">Prof. Hongdong Li</a>
                                    </p>
                                </div>
                            </div>
                            
                            <!-- B.Sci. -->
                            <div class="timeline-item flex gap-4">
                                <div class="school-logo-container w-16 h-16 flex-shrink-0 bg-white rounded-lg shadow-sm p-2 flex items-center justify-center border border-neutral-100">
                                    <img src="assets/logos/Unimelb.png" alt="University of Melbourne" class="w-full h-full object-contain rounded">
                                </div>
                                <div class="timeline-content">
                                    <h3 class="font-semibold text-primary">B.Sci. in Computer Science</h3>
                                    <p class="text-sm text-accent font-medium">The University of Melbourne</p>
                                    <p class="text-sm text-neutral-500">2019.03 - 2021.12</p>
                                </div>
                            </div>
                        </div>
                    </section>

                    <!-- Academic Service Section -->
                    <section id="services" class="scroll-mt-24">
                        <div class="section-header mb-6">
                            <h2 class="text-3xl font-serif font-bold text-primary">üíº Academic Service</h2>
                        </div>
                        <div class="timeline space-y-6">
                            <!-- Reviewer -->
                            <div class="timeline-item flex gap-4">
                                <div class="school-logo-container w-16 h-16 flex-shrink-0 bg-white rounded-lg shadow-sm p-2 flex items-center justify-center border border-neutral-100 text-2xl text-accent">
                                    <i class="fas fa-edit"></i>
                                </div>
                                <div class="timeline-content">
                                    <h3 class="font-semibold text-primary">Conference Reviewer</h3>
                                    <p class="text-sm text-accent font-medium">EMNLP, NAACL, ACMMM, AAAI, ACL, ECCV, CVPR</p>
                                    <p class="text-sm text-neutral-600 mt-1 leading-relaxed">
                                        Served as a reviewer for top-tier computer vision, natural language processing, and machine learning conferences.
                                    </p>
                                </div>
                            </div>
                        </div>
                    </section>

                </div>
            </div>
        </div>
    </main>

    <!-- Image Modal -->
    <div id="image-modal" class="image-modal">
        <span class="image-modal-close">&times;</span>
        <img class="image-modal-content" id="modal-image">
        <div class="image-modal-caption" id="modal-caption"></div>
    </div>

    <!-- Footer -->
    <footer class="bg-white border-t border-neutral-200 py-8 mt-12">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 text-center">
            <p class="text-neutral-500 text-sm">
                &copy; 2026 Tiancheng Gu. All rights reserved. <br>
                Powered by <a href="https://github.com/yihangtao/AcaNova-X" class="text-accent hover:text-accent-dark">AcaNova-X Template</a>
            </p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>
